{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_k: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, n_heads * d_k)\n",
    "        self.W_K = nn.Linear(d_model, n_heads * d_k)\n",
    "        self.W_V = nn.Linear(d_model, n_heads * d_k)\n",
    "        self.output = nn.Linear(n_heads * d_k, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask):\n",
    "        batch_size = query.size(0)\n",
    "        src_len = query.size(1)\n",
    "        d_k = self.d_k\n",
    "        n_heads = self.n_heads\n",
    "\n",
    "        query = self.W_Q(query).reshape(batch_size, src_len, n_heads, d_k).transpose(1, 2)\n",
    "        key = self.W_K(key).reshape(batch_size, src_len, n_heads, d_k).transpose(1, 2)\n",
    "        value = self.W_V(value).reshape(batch_size, src_len, n_heads, d_k).transpose(1, 2)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            assert attn_mask.size() == (batch_size, src_len, src_len)\n",
    "            attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "            attn_mask = attn_mask.bool()\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores.masked_fill_(attn_mask, -1e4)\n",
    "\n",
    "        attns = self.softmax(scores)\n",
    "        output = torch.matmul(attns, value)\n",
    "        output = output.transpose(1, 2).contiguous().reshape(batch_size, -1, d_k * n_heads)\n",
    "        output = self.output(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5374,  0.8965,  1.1350,  ...,  1.6581,  0.1686,  1.3624],\n",
      "         [ 0.5993, -0.4241,  0.6466,  ...,  1.6504,  0.6699,  1.4069],\n",
      "         [-1.6111,  0.5703,  0.5384,  ...,  3.2205, -0.7980, -0.4078],\n",
      "         ...,\n",
      "         [-0.0642, -0.8640, -1.7854,  ...,  0.0496, -0.1097, -0.0803],\n",
      "         [ 0.3494,  0.5224, -1.7362,  ..., -0.7658,  1.4892,  0.4058],\n",
      "         [-0.9695,  1.0183, -3.5471,  ..., -0.4584, -0.8055,  0.2513]],\n",
      "\n",
      "        [[-0.9149,  0.9571,  0.4021,  ..., -0.5145, -0.2525, -0.5677],\n",
      "         [-0.9343,  1.0164,  1.0843,  ...,  0.9977, -0.5970,  0.5019],\n",
      "         [-0.7378, -2.5707, -1.7871,  ..., -0.5649,  0.5830,  0.3226],\n",
      "         ...,\n",
      "         [-0.2303, -1.5828, -0.9122,  ..., -0.2734, -1.0095,  0.1967],\n",
      "         [ 0.7680, -0.1118,  0.4220,  ..., -0.2372,  0.0644,  1.7829],\n",
      "         [ 0.6531, -1.6374, -0.9833,  ..., -0.1718, -1.0883, -0.2823]],\n",
      "\n",
      "        [[-1.1870, -0.1303,  0.7256,  ..., -0.3316,  0.9571,  0.8577],\n",
      "         [-1.0492, -1.2005, -0.0238,  ..., -1.1428,  0.0194, -1.3721],\n",
      "         [ 0.5047, -0.1232,  2.2771,  ..., -0.0784, -1.1107, -0.3092],\n",
      "         ...,\n",
      "         [-1.2335, -0.6219,  0.2479,  ...,  1.2432, -0.1951, -0.9783],\n",
      "         [ 1.8141,  0.5978,  0.2082,  ..., -0.0072, -0.2395, -1.3965],\n",
      "         [-1.2519, -0.2455, -1.2510,  ...,  1.0008, -1.5339,  1.4511]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.8221, -0.1721, -0.6345,  ..., -1.2580,  2.2318, -1.5709],\n",
      "         [ 0.4170, -0.9953, -0.8245,  ...,  1.0019,  0.3697, -0.3919],\n",
      "         [-0.8649,  0.0646, -0.3455,  ...,  0.2871, -0.8847, -1.2232],\n",
      "         ...,\n",
      "         [ 0.2419, -1.7518, -0.8669,  ...,  1.6902,  0.0598,  0.1335],\n",
      "         [-0.0667, -0.5230,  0.3611,  ...,  0.6489,  0.5059,  0.9953],\n",
      "         [ 0.3512, -0.7890,  0.3303,  ...,  0.7025,  0.1221,  0.5735]],\n",
      "\n",
      "        [[ 0.9262,  0.4559,  0.4221,  ...,  0.6336, -0.2444,  1.8674],\n",
      "         [ 0.7701,  0.6892,  1.6138,  ...,  0.2083, -1.0356,  0.0494],\n",
      "         [-0.1769,  1.6433,  0.4274,  ..., -0.9825, -0.3626, -1.8410],\n",
      "         ...,\n",
      "         [ 0.8146, -1.3494, -1.1675,  ...,  0.1025, -1.8099,  0.3091],\n",
      "         [-0.4791, -0.1965,  1.0801,  ...,  1.9027, -0.4241,  1.2548],\n",
      "         [ 0.0235, -0.0286, -0.0492,  ...,  0.1056, -0.1193,  0.5401]],\n",
      "\n",
      "        [[ 1.1188,  0.5255,  0.0229,  ..., -0.2314,  0.2735, -1.6351],\n",
      "         [ 0.6266,  0.0775, -0.4961,  ...,  1.8775,  0.3113, -1.2600],\n",
      "         [-1.1962, -1.4601,  0.4745,  ..., -0.0077,  1.0787, -0.6270],\n",
      "         ...,\n",
      "         [ 1.5055,  1.2363,  0.2629,  ...,  1.0567,  0.1395, -0.0689],\n",
      "         [ 0.3855, -0.8381, -0.9544,  ...,  1.2981,  2.1427, -0.3449],\n",
      "         [-0.2381,  0.4940, -2.2285,  ..., -0.5255,  1.6601, -2.2356]]])\n",
      "tensor([[[ 0.0825,  0.0945, -0.0184,  ..., -0.0167,  0.0778,  0.0460],\n",
      "         [ 0.0476,  0.0799, -0.0016,  ..., -0.0381,  0.0685,  0.0306],\n",
      "         [ 0.0648,  0.0925, -0.0225,  ..., -0.0191,  0.0732,  0.0210],\n",
      "         ...,\n",
      "         [ 0.0763,  0.1069, -0.0114,  ..., -0.0052,  0.0474,  0.0320],\n",
      "         [ 0.0720,  0.1004, -0.0198,  ..., -0.0036,  0.0574,  0.0317],\n",
      "         [ 0.0546,  0.1013, -0.0093,  ..., -0.0178,  0.0370,  0.0395]],\n",
      "\n",
      "        [[ 0.0119,  0.0680, -0.0314,  ...,  0.0334,  0.0300,  0.0324],\n",
      "         [ 0.0224,  0.0799, -0.0132,  ...,  0.0446,  0.0185,  0.0634],\n",
      "         [ 0.0097,  0.0672, -0.0256,  ...,  0.0426,  0.0357,  0.0471],\n",
      "         ...,\n",
      "         [ 0.0138,  0.0710, -0.0283,  ...,  0.0465,  0.0295,  0.0534],\n",
      "         [ 0.0296,  0.0662, -0.0371,  ...,  0.0199,  0.0434,  0.0479],\n",
      "         [ 0.0178,  0.0631, -0.0427,  ...,  0.0618,  0.0354,  0.0358]],\n",
      "\n",
      "        [[ 0.0394, -0.0031, -0.0478,  ...,  0.0675,  0.0171, -0.0404],\n",
      "         [ 0.0713,  0.0176, -0.0332,  ...,  0.0659,  0.0154, -0.0284],\n",
      "         [ 0.0350,  0.0266, -0.0479,  ...,  0.0536,  0.0357, -0.0299],\n",
      "         ...,\n",
      "         [ 0.0596,  0.0191, -0.0545,  ...,  0.0439,  0.0275, -0.0409],\n",
      "         [ 0.0230,  0.0210, -0.0414,  ...,  0.0785,  0.0285, -0.0263],\n",
      "         [ 0.0328,  0.0256, -0.0465,  ...,  0.0595,  0.0458,  0.0015]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0137,  0.0669,  0.0382,  ...,  0.0494,  0.0559, -0.0163],\n",
      "         [-0.0099,  0.0709,  0.0495,  ...,  0.0590,  0.0605, -0.0324],\n",
      "         [-0.0181,  0.0801,  0.0325,  ...,  0.0247,  0.0439, -0.0173],\n",
      "         ...,\n",
      "         [-0.0211,  0.0581,  0.0578,  ...,  0.0584,  0.0491, -0.0304],\n",
      "         [-0.0401,  0.0777,  0.0534,  ...,  0.0335,  0.0455, -0.0347],\n",
      "         [-0.0330,  0.0559,  0.0568,  ...,  0.0476,  0.0442, -0.0146]],\n",
      "\n",
      "        [[-0.0141,  0.0387, -0.0394,  ...,  0.0418,  0.0260,  0.0383],\n",
      "         [-0.0463,  0.0619, -0.0298,  ...,  0.0590,  0.0430,  0.0339],\n",
      "         [-0.0256,  0.0466, -0.0354,  ...,  0.0442,  0.0320,  0.0092],\n",
      "         ...,\n",
      "         [-0.0265,  0.0495, -0.0529,  ...,  0.0587,  0.0265,  0.0273],\n",
      "         [-0.0443,  0.0312, -0.0420,  ...,  0.0320,  0.0308,  0.0272],\n",
      "         [-0.0328,  0.0364, -0.0191,  ...,  0.0594,  0.0312,  0.0404]],\n",
      "\n",
      "        [[ 0.0004,  0.0718,  0.0078,  ...,  0.0296,  0.0177,  0.0167],\n",
      "         [-0.0235,  0.0846,  0.0381,  ...,  0.0170,  0.0270,  0.0385],\n",
      "         [-0.0023,  0.0647,  0.0215,  ...,  0.0292,  0.0183,  0.0299],\n",
      "         ...,\n",
      "         [-0.0061,  0.0661,  0.0333,  ...,  0.0177,  0.0143,  0.0263],\n",
      "         [-0.0127,  0.0672,  0.0302,  ...,  0.0295,  0.0449,  0.0414],\n",
      "         [-0.0068,  0.0761,  0.0213,  ...,  0.0199,  0.0234,  0.0276]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "MHA = MultiHeadAttention(  \n",
    "    d_model=512,   # 模型总维度  \n",
    "    n_heads=8,     # 8个注意力头  \n",
    "    d_k=64         # 每个头的维度  \n",
    ")  \n",
    "# 使用示例  \n",
    "batch_size = 32  \n",
    "seq_length = 100  \n",
    "query = torch.randn(batch_size, seq_length, 512)  \n",
    "key = torch.randn(batch_size, seq_length, 512)  \n",
    "value = torch.randn(batch_size, seq_length, 512)  \n",
    "\n",
    "# 创建注意力遮罩(可选)  \n",
    "attn_mask = torch.zeros(batch_size, seq_length, seq_length).bool()  \n",
    "\n",
    "# 前向传播  \n",
    "output = MHA(query, key, value, attn_mask)  \n",
    "print(query)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

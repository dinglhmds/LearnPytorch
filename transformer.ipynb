{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len_mask(b: int, max_len: int, feat_lens: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
    "    attn_mask = torch.ones((b, max_len, max_len), device=device)\n",
    "    for i in range(b):\n",
    "        attn_mask[i, :, :feat_lens[i]] = 0\n",
    "    return attn_mask.to(torch.bool)\n",
    "def get_subsequent_mask(b: int, max_len: int, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        b: batch-size.\n",
    "        max_len: the length of the whole seqeunce.\n",
    "        device: cuda or cpu.\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones((b, max_len, max_len), device=device), diagonal=1).to(torch.bool)\n",
    "def get_enc_dec_mask(\n",
    "    b: int, max_feat_len: int, feat_lens: torch.Tensor, max_label_len: int, device: torch.device\n",
    ") -> torch.Tensor:\n",
    "    attn_mask = torch.zeros((b, max_label_len, max_feat_len), device=device)       # (b, seq_q, seq_k)\n",
    "    for i in range(b):\n",
    "        attn_mask[i, :, feat_lens[i]:] = 1\n",
    "    return attn_mask.to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_v, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, n_heads * d_k)\n",
    "        self.W_K = nn.Linear(d_model, n_heads * d_k)\n",
    "        self.W_V = nn.Linear(d_model, n_heads * d_v)\n",
    "        self.output = nn.Linear(n_heads * d_v, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask):\n",
    "        batch_size = query.size(0)\n",
    "        src_len = query.size(1)\n",
    "        k_len = key.size(1)\n",
    "        d_k = self.d_k\n",
    "        d_v = self.d_v\n",
    "        n_heads = self.n_heads\n",
    "\n",
    "        query = self.W_Q(query).reshape(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
    "        key = self.W_K(key).reshape(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
    "        value = self.W_V(value).reshape(batch_size, -1, n_heads, d_v).transpose(1, 2)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            assert attn_mask.size() == (batch_size, src_len, k_len)\n",
    "            attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "            attn_mask = attn_mask.bool()\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores.masked_fill_(attn_mask, -1e4)\n",
    "\n",
    "        attns = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attns, value)\n",
    "        output = output.transpose(1, 2).contiguous().reshape(batch_size, -1, d_v * n_heads)\n",
    "        output = self.output(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-4.2775e-01,  1.1009e+00, -5.5461e-01,  ...,  7.1175e-01,\n",
      "          -1.2172e+00, -4.7280e-01],\n",
      "         [-7.3395e-02,  1.7238e+00, -1.5250e+00,  ...,  9.3528e-01,\n",
      "           2.0412e-01,  3.5094e-01],\n",
      "         [ 1.4875e+00,  6.2181e-02, -1.9177e-01,  ...,  2.3646e-01,\n",
      "           1.6656e-01, -7.1866e-01],\n",
      "         ...,\n",
      "         [-6.5702e-02, -4.2091e-01,  9.0087e-01,  ...,  4.3691e-01,\n",
      "           3.1007e-01, -6.0949e-01],\n",
      "         [ 5.8539e-02, -8.0803e-01,  7.5208e-01,  ..., -3.4936e-01,\n",
      "           7.7019e-01,  4.4988e-01],\n",
      "         [-6.9764e-02,  3.9726e-02, -4.1676e-02,  ...,  2.1024e+00,\n",
      "           3.1992e-01,  1.8180e+00]],\n",
      "\n",
      "        [[ 8.6516e-01,  1.8501e-01,  7.2429e-01,  ..., -1.0912e+00,\n",
      "          -8.1465e-01,  1.4580e+00],\n",
      "         [-4.2022e-01,  2.3325e-01, -5.5296e-01,  ...,  1.9702e-01,\n",
      "           1.6790e+00,  6.5472e-01],\n",
      "         [ 1.2830e+00, -1.4826e+00, -2.6093e-01,  ..., -3.1383e-02,\n",
      "           6.1435e-01, -7.1637e-01],\n",
      "         ...,\n",
      "         [-9.5878e-01, -4.0409e-01,  1.3190e+00,  ...,  6.9325e-01,\n",
      "           4.0497e-01,  1.0177e-01],\n",
      "         [ 8.6811e-01, -5.9149e-01, -6.5329e-01,  ..., -7.7438e-01,\n",
      "          -6.4386e-01, -9.1124e-02],\n",
      "         [ 9.9841e-01, -1.4490e-01,  2.2531e-02,  ..., -2.1404e+00,\n",
      "           3.0509e-01,  6.2263e-01]],\n",
      "\n",
      "        [[-1.3344e-01, -7.3835e-01,  5.6042e-01,  ...,  3.0752e-01,\n",
      "          -1.7434e-01,  5.5792e-01],\n",
      "         [ 9.0781e-01, -2.6284e+00, -1.4004e+00,  ..., -1.6558e+00,\n",
      "           3.6732e-01, -1.2575e+00],\n",
      "         [ 1.2314e+00,  4.1581e-01,  3.7171e-01,  ..., -6.8748e-01,\n",
      "          -2.2215e-02,  6.9348e-02],\n",
      "         ...,\n",
      "         [-1.0897e+00,  3.5053e-01, -6.4239e-01,  ...,  8.3598e-01,\n",
      "          -9.5346e-01, -9.6051e-01],\n",
      "         [ 4.6379e-01, -1.7284e+00,  1.1783e+00,  ..., -1.1183e+00,\n",
      "           1.4140e+00, -5.4011e-01],\n",
      "         [ 9.3954e-02, -9.6595e-01,  7.0243e-02,  ..., -9.0767e-02,\n",
      "           6.3537e-01,  2.4481e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.3278e-01, -2.1085e+00, -3.6775e-02,  ..., -5.4372e-01,\n",
      "          -1.7436e-01,  6.2201e-01],\n",
      "         [-9.7462e-02,  1.3599e+00,  9.5870e-03,  ..., -5.2093e-01,\n",
      "          -3.4459e-01,  6.7411e-01],\n",
      "         [ 6.5903e-01, -2.5397e-01,  9.2965e-01,  ...,  2.3794e-01,\n",
      "           1.7457e-01, -4.7936e-01],\n",
      "         ...,\n",
      "         [-1.3385e-01,  2.1053e+00,  1.9778e-03,  ..., -2.2383e-01,\n",
      "           1.5714e+00, -8.1321e-01],\n",
      "         [-4.4259e-02,  6.8582e-01,  3.2000e-02,  ..., -1.2323e+00,\n",
      "           9.1286e-01, -5.5875e-01],\n",
      "         [-5.0702e-01,  1.5959e-01,  1.0452e+00,  ...,  5.3505e-01,\n",
      "           1.1638e+00, -1.8005e-02]],\n",
      "\n",
      "        [[-9.9176e-01, -9.1465e-01,  5.0821e-01,  ..., -3.6156e-01,\n",
      "           1.7932e+00,  7.4493e-01],\n",
      "         [-2.0529e-01, -1.4405e+00, -1.2252e+00,  ...,  8.0853e-01,\n",
      "           1.2031e-01,  3.3069e-01],\n",
      "         [-2.7619e-01, -2.2945e+00, -5.5194e-01,  ...,  7.8357e-01,\n",
      "           1.1449e+00,  1.1442e+00],\n",
      "         ...,\n",
      "         [ 5.6010e-01,  2.0980e+00,  2.2526e-02,  ..., -3.6756e-01,\n",
      "           9.2701e-01, -1.1736e+00],\n",
      "         [ 9.5232e-01, -1.7406e+00,  5.1541e-01,  ...,  9.9657e-01,\n",
      "          -1.2661e+00, -1.2285e+00],\n",
      "         [ 7.1146e-01,  7.6165e-01,  9.0742e-01,  ...,  1.3121e+00,\n",
      "          -1.0725e+00,  4.0126e-01]],\n",
      "\n",
      "        [[ 9.4308e-01, -1.5581e+00, -2.1718e-01,  ..., -3.2479e-01,\n",
      "           7.8473e-02, -4.9799e-01],\n",
      "         [ 1.6219e+00,  1.3384e+00, -2.5655e+00,  ..., -4.4215e-01,\n",
      "          -2.0893e+00, -1.5618e-01],\n",
      "         [-1.6948e+00, -1.2234e+00,  7.5165e-01,  ..., -4.4757e-01,\n",
      "          -1.7739e-01,  1.0540e+00],\n",
      "         ...,\n",
      "         [-1.2379e-01,  3.7419e-01,  7.3812e-01,  ..., -1.4652e+00,\n",
      "          -1.5527e+00, -6.9010e-01],\n",
      "         [ 4.1349e-01,  1.0026e+00, -1.2466e+00,  ..., -3.0298e-01,\n",
      "          -3.2358e-01,  1.0912e+00],\n",
      "         [-4.4405e-02,  2.7610e-01,  1.1319e+00,  ..., -2.4992e+00,\n",
      "          -9.0591e-01, -7.2583e-02]]])\n",
      "tensor([[[ 8.4568e-02, -3.3595e-02,  4.9751e-02,  ..., -1.8796e-02,\n",
      "          -3.6419e-02,  1.0230e-02],\n",
      "         [ 7.7162e-02, -3.9353e-02,  2.5901e-02,  ..., -2.9734e-02,\n",
      "          -2.5139e-02,  4.8698e-02],\n",
      "         [ 9.2246e-02, -3.5016e-02,  1.7227e-02,  ..., -2.5345e-02,\n",
      "          -3.5028e-02,  5.0401e-02],\n",
      "         ...,\n",
      "         [ 7.1313e-02, -3.0699e-02,  3.2381e-02,  ..., -2.5473e-02,\n",
      "           7.9611e-03,  2.1329e-02],\n",
      "         [ 9.0532e-02, -2.8836e-02,  5.2797e-02,  ..., -1.7378e-02,\n",
      "          -2.8619e-02,  3.4815e-02],\n",
      "         [ 1.2181e-01, -2.2411e-02,  3.8984e-02,  ..., -3.8875e-02,\n",
      "          -4.0135e-02,  2.3635e-02]],\n",
      "\n",
      "        [[ 9.1999e-02, -2.9993e-03,  3.8450e-02,  ...,  1.9579e-02,\n",
      "          -9.0360e-03,  3.4239e-02],\n",
      "         [ 7.9407e-02, -3.6833e-03,  6.5915e-02,  ...,  1.3384e-02,\n",
      "          -3.0844e-02,  2.5483e-02],\n",
      "         [ 6.1786e-02, -2.6668e-04,  4.1955e-02,  ...,  1.1941e-02,\n",
      "          -1.6082e-02,  3.9150e-02],\n",
      "         ...,\n",
      "         [ 5.6224e-02,  1.6053e-02,  3.0347e-02,  ...,  1.4511e-02,\n",
      "          -2.1220e-02,  2.5352e-02],\n",
      "         [ 8.1559e-02,  1.6058e-02,  5.4387e-02,  ...,  1.7354e-02,\n",
      "          -7.7786e-03,  6.3998e-02],\n",
      "         [ 5.6681e-02,  1.0004e-04,  4.1633e-02,  ...,  2.0903e-02,\n",
      "          -2.2766e-03,  3.0380e-02]],\n",
      "\n",
      "        [[ 1.7545e-02, -3.0625e-02,  7.1496e-02,  ...,  1.9552e-02,\n",
      "          -3.2483e-02,  3.3825e-02],\n",
      "         [ 5.1818e-02, -3.2718e-02,  7.3568e-02,  ...,  1.6081e-02,\n",
      "          -4.4550e-02,  3.1918e-02],\n",
      "         [ 5.3665e-02, -2.2219e-02,  8.6074e-02,  ...,  1.7121e-02,\n",
      "          -2.4516e-02,  2.9537e-02],\n",
      "         ...,\n",
      "         [ 6.5372e-02, -4.3597e-02,  4.7867e-02,  ...,  9.2793e-03,\n",
      "          -3.1802e-02,  4.1762e-02],\n",
      "         [ 6.3411e-02, -2.0783e-02,  5.0093e-02,  ...,  1.0935e-02,\n",
      "          -4.1461e-02,  1.7874e-02],\n",
      "         [ 5.0457e-02, -3.8499e-02,  6.5169e-02,  ...,  1.0932e-02,\n",
      "          -5.2372e-02,  1.7425e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1467e-02,  1.5103e-02,  2.7370e-02,  ...,  4.1916e-02,\n",
      "          -5.7519e-02,  4.3658e-02],\n",
      "         [ 1.0208e-02,  7.1082e-03,  1.6432e-02,  ...,  2.9844e-02,\n",
      "          -5.4024e-02,  2.6848e-02],\n",
      "         [-2.1288e-03,  1.6739e-02,  2.5761e-03,  ...,  5.7857e-02,\n",
      "          -7.3532e-02,  4.9269e-02],\n",
      "         ...,\n",
      "         [ 1.7577e-02,  3.5199e-02,  3.5018e-02,  ...,  4.2220e-02,\n",
      "          -6.7018e-02,  3.1574e-02],\n",
      "         [ 1.8978e-02,  1.1538e-02,  1.2792e-02,  ...,  2.8336e-02,\n",
      "          -4.2061e-02,  5.1163e-02],\n",
      "         [ 2.9874e-02, -6.0076e-04,  1.8160e-02,  ...,  3.7817e-02,\n",
      "          -5.2868e-02,  3.7460e-02]],\n",
      "\n",
      "        [[ 1.0466e-02, -1.5170e-02,  4.6059e-02,  ...,  2.7290e-02,\n",
      "          -9.9206e-02,  5.2307e-02],\n",
      "         [-2.4612e-03, -2.3915e-02,  5.6040e-02,  ...,  3.0522e-02,\n",
      "          -7.4054e-02,  5.1981e-02],\n",
      "         [ 9.1745e-03,  6.8688e-03,  2.5175e-02,  ...,  2.3963e-02,\n",
      "          -1.0352e-01,  5.2369e-02],\n",
      "         ...,\n",
      "         [ 4.4945e-04,  1.5465e-02,  1.4380e-02,  ...,  4.4106e-02,\n",
      "          -9.6895e-02,  5.1384e-02],\n",
      "         [-1.4056e-02, -1.4911e-03,  3.4914e-02,  ...,  3.4578e-02,\n",
      "          -7.4965e-02,  4.7198e-02],\n",
      "         [-8.7797e-03, -5.7659e-03,  2.7343e-02,  ...,  4.7100e-02,\n",
      "          -9.2892e-02,  7.5182e-02]],\n",
      "\n",
      "        [[ 1.6250e-02, -1.1446e-02,  4.5061e-02,  ...,  6.3694e-02,\n",
      "          -2.5609e-02, -3.2403e-02],\n",
      "         [ 3.4281e-02, -4.7899e-02,  6.6652e-02,  ...,  8.9676e-02,\n",
      "           1.4941e-03, -3.7353e-02],\n",
      "         [ 2.3638e-02, -7.8937e-03,  5.5082e-02,  ...,  6.4660e-02,\n",
      "          -2.7944e-02, -9.7487e-03],\n",
      "         ...,\n",
      "         [ 3.0984e-02, -4.5886e-02,  5.6097e-02,  ...,  7.9512e-02,\n",
      "          -1.6373e-02, -2.8330e-02],\n",
      "         [ 2.9563e-02, -1.9735e-02,  5.7238e-02,  ...,  7.4816e-02,\n",
      "          -1.4496e-02, -1.2900e-02],\n",
      "         [ 5.2582e-02, -3.7938e-02,  4.9563e-02,  ...,  7.0628e-02,\n",
      "          -2.2018e-02, -1.2399e-02]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "MHA = MultiHeadAttention(  \n",
    "    d_k=64,        # 每个头的维度  \n",
    "    d_v=64,\n",
    "    d_model=512,   # 模型总维度  \n",
    "    n_heads=8,     # 8个注意力头  \n",
    "    \n",
    ")  \n",
    "# 使用示例  \n",
    "batch_size = 32  \n",
    "seq_length = 100  \n",
    "query = torch.randn(batch_size, seq_length, 512)  \n",
    "key = torch.randn(batch_size, seq_length, 512)  \n",
    "value = torch.randn(batch_size, seq_length, 512)  \n",
    "\n",
    "# 创建注意力遮罩(可选)  \n",
    "attn_mask = torch.zeros(batch_size, seq_length, seq_length).bool()  \n",
    "\n",
    "# 前向传播  \n",
    "output = MHA(query, key, value, attn_mask)  \n",
    "print(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(src_len, d_model):\n",
    "    embedding_table = torch.zeros((src_len, d_model))\n",
    "    for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "            f = torch.sin\n",
    "        else:\n",
    "            f = torch.cos\n",
    "        embedding_table[::, i] = f(torch.arange(0, src_len) / np.power(10000, 2 * (i // 2) / d_model))\n",
    "    return embedding_table.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.conv1 = nn.Conv1d(d_model, d_ff, 1, 1, 0)\n",
    "        self.conv2 = nn.Conv1d(d_ff, d_model, 1, 1, 0)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # output of MHA (batch_size, src_len, d_model)\n",
    "        out = self.conv1(x.transpose(1, 2))\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out).transpose(1, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dff):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: input dimension\n",
    "            n_heads: number of attention heads\n",
    "            dff: dimension f PosFFN (Positional FeedForward)\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        d_k = d_model // n_heads\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.multi_head_attn = MultiHeadAttention(d_k, d_k, d_model, n_heads)\n",
    "\n",
    "        self.poswise_ffn = PoswiseFFN(d_model, dff)\n",
    "\n",
    "    def forward(self, enc_in, attn_mask):\n",
    "        residual = enc_in\n",
    "\n",
    "        out = self.multi_head_attn(enc_in, enc_in, enc_in, attn_mask)\n",
    "        out = self.norm1(residual + out)\n",
    "        residual = out\n",
    "        out = self.poswise_ffn(out)\n",
    "        out = self.norm2(residual + out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, enc_dim, n_heads, dff, tgt_len):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_layers: numer of encoder layers\n",
    "            enc_dim: input dimension of encoder\n",
    "            n_heads: number of attention heads\n",
    "            dff: dimension of PosFFN\n",
    "            tgt_len: the maximum length of sequences\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.tgt_len = tgt_len\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(position_embedding(tgt_len, enc_dim), freeze=True)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(enc_dim, n_heads, dff) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_lens, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        out = x + self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dff):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: input dimension\n",
    "            n_heads: number of attention heads\n",
    "            dff: dimension f PosFFN (Positional FeedForward)\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        d_k = d_model // n_heads\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.poswise_ffn = PoswiseFFN(d_model, dff)\n",
    "\n",
    "        self.dec_attn = MultiHeadAttention(d_k, d_k, d_model, n_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_k, d_k, d_model, n_heads)\n",
    "\n",
    "    def forward(self, dec_in, enc_out, dec_mask, dec_enc_mask):\n",
    "        residual = dec_in\n",
    "        context = self.dec_attn(dec_in, dec_in, dec_in, dec_mask)\n",
    "        dec_out = self.norm1(residual + context)\n",
    "        residual = dec_out\n",
    "        context = self.enc_dec_attn(dec_out, enc_out, enc_out, dec_enc_mask)\n",
    "        dec_out = self.norm2(residual + context)\n",
    "        residual = dec_out\n",
    "        out = self.poswise_ffn(dec_out)\n",
    "        dec_out = self.norm3(residual + out)\n",
    "        return dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, dec_dim, n_heads, dff, tgt_len, tgt_vocab_size):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_layers: number of encoder layers\n",
    "            dec_dim: input dimension of decoder\n",
    "            num_heads: number of attention heads\n",
    "            dff: dimensionf of PosFFN\n",
    "            tgt_len: the target length to be embedded.\n",
    "            tgt_vocab_size: the target vocabulary size.\n",
    "        \"\"\"\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, dec_dim)\n",
    "\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(position_embedding(tgt_len, dec_dim), freeze=True)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(dec_dim, n_heads, dff) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self,  labels, enc_out, dec_mask, dec_enc_mask):\n",
    "        tgt_emb = self.tgt_emb(labels)\n",
    "        pos_emb = self.pos_emb(torch.arange(labels.size(1), device=labels.device))\n",
    "        dec_out = tgt_emb + pos_emb\n",
    "        for layer in self.layers:\n",
    "            dec_out = layer(dec_out, enc_out, dec_mask, dec_enc_mask)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, frontend: nn.Module, encoder: nn.Module, decoder: nn.Module,\n",
    "            dec_out_dim: int, vocab: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.frontend = frontend     # feature extractor\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.linear = nn.Linear(dec_out_dim, vocab)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, X_lens: torch.Tensor, labels: torch.Tensor):\n",
    "        X_lens, labels = X_lens.long(), labels.long()\n",
    "        b = X.size(0)\n",
    "        device = X.device\n",
    "        # frontend\n",
    "        out = self.frontend(X)\n",
    "        max_feat_len = out.size(1)                            # compute after frontend because of optional subsampling\n",
    "        max_label_len = labels.size(1)\n",
    "        # encoder\n",
    "        enc_mask = get_len_mask(b, max_feat_len, X_lens, device)\n",
    "        enc_out = self.encoder(out, X_lens, enc_mask)\n",
    "        # decoder\n",
    "        dec_mask = get_subsequent_mask(b, max_label_len, device)\n",
    "        dec_enc_mask = get_enc_dec_mask(b, max_feat_len, X_lens, max_label_len, device)\n",
    "        dec_out = self.decoder(labels, enc_out, dec_mask, dec_enc_mask)\n",
    "        logits = self.linear(dec_out)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([16, 50, 26])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# constants\n",
    "batch_size = 16                 # batch size\n",
    "max_feat_len = 100              # the maximum length of input sequence\n",
    "max_label_len = 50              # the maximum length of output sequence\n",
    "fbank_dim = 80                  # the dimension of input feature\n",
    "hidden_dim = 512                # the dimension of hidden layer\n",
    "vocab_size = 26                 # the size of vocabulary\n",
    "\n",
    "# dummy data\n",
    "fbank_feature = torch.randn(batch_size, max_feat_len, fbank_dim)        # input sequence\n",
    "feat_lens = torch.randint(1, max_feat_len, (batch_size,))               # the length of each input sequence in the batch\n",
    "labels = torch.randint(0, vocab_size, (batch_size, max_label_len))      # output sequence\n",
    "label_lens = torch.randint(1, max_label_len, (batch_size,))             # the length of each output sequence in the batch\n",
    "\n",
    "# model\n",
    "feature_extractor = nn.Linear(fbank_dim, hidden_dim)                    # alinear layer to simulate the audio feature extractor\n",
    "encoder = Encoder(\n",
    "    num_layers=6, enc_dim=hidden_dim, n_heads=8, dff=2048, tgt_len=2048\n",
    ")\n",
    "decoder = Decoder(\n",
    "    num_layers=6, dec_dim=hidden_dim, n_heads=8, dff=2048, tgt_len=2048, tgt_vocab_size=vocab_size\n",
    ")\n",
    "transformer = Transformer(feature_extractor, encoder, decoder, hidden_dim, vocab_size)\n",
    "\n",
    "# forward check\n",
    "logits = transformer(fbank_feature, feat_lens, labels)\n",
    "print(f\"logits: {logits.shape}\")     # (batch_size, max_label_len, vocab_size)\n",
    "\n",
    "# output msg\n",
    "# logits: torch.Size([16, 100, 26])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
